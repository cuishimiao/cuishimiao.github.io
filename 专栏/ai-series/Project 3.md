
# ç”¨PyTorchæ„å»ºæƒ…æ„Ÿåˆ†ææ¨¡å‹



ğŸ­ **æƒ…æ„Ÿåˆ†æç‚¼ä¸¹æŒ‡å—ï¼šç”¨PyTorchç†¬åˆ¶"æƒ…ç»ªæ¢æµ‹å™¨"**
â€”â€”ä»æ•°æ®é¢„å¤„ç†åˆ°æ¨¡å‹è®­ç»ƒã®é­”æ³•å…¨æµç¨‹

---

### â… . **ä»»åŠ¡èƒŒæ™¯ï¼šAIè¯»å¿ƒæœ¯é€Ÿæˆç­**
**ä¸šåŠ¡éœ€æ±‚**ï¼š
- è¾“å…¥å¥å­ â†’ è¾“å‡ºæƒ…ç»ªææ€§ï¼ˆæ­£é¢/è´Ÿé¢ï¼‰
- ç¤ºä¾‹ï¼š
  "è¿™æ‰‹æœºç‰›é€¼ï¼" â†’ ğŸ˜„ (æ­£é¢)
  "å®¢æœæ€åº¦åƒåƒµå°¸" â†’ ğŸ˜  (è´Ÿé¢)

**æŠ€æœ¯é€‰å‹**ï¼š
```
æ–¹æ¡ˆ = è¯å‘é‡(emojiåŒ–) + LSTM(æƒ…ç»ªæ•æ‰‹) + å…¨è¿æ¥(æƒ…æ„Ÿè£åˆ¤)
```

**çµé­‚åæ§½**ï¼š
"æƒ…æ„Ÿåˆ†æå°±åƒç»™AIè£…äº†ä¸ªæµ‹è°ä»ªï¼Œä¸“æ²»ç½‘ä¸Šé˜´é˜³æ€ªæ°”ï¼"

---

### â…¡. **æ•°æ®é¢„å¤„ç†ï¼šåˆ¶ä½œæƒ…ç»ªé­”æ³•è¯å‰‚**
#### 1. åŠ è½½IMDBç”µå½±è¯„è®ºæ•°æ®é›†ï¼ˆè‡ªå¸¦æ­£è´Ÿæ ‡ç­¾ï¼‰
```python
from torchtext.datasets import IMDB

# æŸ¥çœ‹æ•°æ®æ ·ä¾‹
train_data = IMDB(split='train')
print(next(iter(train_data)))  # è¾“å‡º: ("This film is terrible...", 0)
```

#### 2. æ–‡æœ¬åˆ†è¯ä¸è¯è¡¨æ„å»ºï¼ˆä½¿ç”¨spacyåŠ é€Ÿï¼‰
```python
import spacy
nlp = spacy.load('en_core_web_sm')

def tokenizer(text):
    return [token.text for token in nlp(text)]

from torchtext.vocab import build_vocab_from_iterator

# æ„å»ºè¯è¡¨ï¼ˆè‡ªåŠ¨è¿‡æ»¤ä½é¢‘è¯ï¼‰
vocab = build_vocab_from_iterator(
    map(tokenizer, [text for text, label in train_data]),
    min_freq=5,
    specials=['<unk>', '<pad>']
)
vocab.set_default_index(vocab['<unk>'])

print("è¯è¡¨å¤§å°:", len(vocab))  # è¾“å‡º: çº¦25000
```

**é­”æ³•æ³¨é‡Š**ï¼š
- `<unk>`: æœªçŸ¥è¯å ä½ç¬¦ï¼ˆé‡åˆ°ç”Ÿåƒ»è¯æ—¶çš„ä¸‡èƒ½èƒ¶ï¼‰
- `<pad>`: å¡«å……ç¬¦ï¼ˆåƒæ–¹ä¾¿é¢é‡Œçš„è„±æ°´è”¬èœï¼Œå……æ•°ç”¨çš„ï¼‰

---

### â…¢. **æ•°æ®ç®¡é“ï¼šæ‰“é€ æ–‡æœ¬æµæ°´çº¿**
#### 1. æ–‡æœ¬å‘é‡åŒ–å‡½æ•°ï¼ˆæ–‡å­—â†’æ•°å­—ï¼‰
```python
text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]
label_pipeline = lambda x: 1 if x == 'pos' else 0

# æµ‹è¯•æ•ˆæœ
print(text_pipeline("I love PyTorch!"))  # è¾“å‡º: [23, 56, 345]
print(label_pipeline('pos'))             # è¾“å‡º: 1
```

#### 2. å°è£…DataLoaderï¼ˆè‡ªåŠ¨åˆ†æ‰¹+å¡«å……ï¼‰
```python
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence

def collate_batch(batch):
    text_list, label_list = [], []
    for (_text, _label) in batch:
        text = torch.tensor(text_pipeline(_text), dtype=torch.int64)
        text_list.append(text)
        label_list.append(label_pipeline(_label))
  
    # å¡«å……åˆ°ç›¸åŒé•¿åº¦ï¼ˆåƒæ•´ç†ä¸åŒé«˜åº¦çš„ä¹¦æ¶ï¼‰
    padded_text = pad_sequence(text_list, padding_value=vocab['<pad>'])
    return padded_text.T, torch.tensor(label_list)

# åˆ›å»ºDataLoader
train_loader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collate_batch)
```

**æŠ€æœ¯åæ§½**ï¼š
"pad_sequenceå°±åƒç»™ä¸åŒèº«é«˜çš„å­¦ç”Ÿå‘å¢é«˜é‹å«ï¼"

---

### â…£. **æ¨¡å‹æ„å»ºï¼šç»„è£…æƒ…ç»ªåˆ†ææœºç”²**
```python
import torch.nn as nn

class EmotionDetector(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_size):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab['<pad>'])
        self.lstm = nn.LSTM(embed_dim, hidden_size, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_size*2, 1)  # åŒå‘LSTMéœ€è¦*2
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        # xå½¢çŠ¶: [batch_size, seq_len]
        embedded = self.embedding(x)          # â†’ [batch, seq, embed]
        lstm_out, _ = self.lstm(embedded)     # â†’ [batch, seq, hidden*2]
        # å–æœ€åæ—¶åˆ»çš„è¾“å‡ºï¼ˆæƒ…ç»ªç´¯ç§¯ç»“æœï¼‰
        last_output = lstm_out[:, -1, :]      # â†’ [batch, hidden*2]
        return self.fc(self.dropout(last_output))

# åˆå§‹åŒ–æ¨¡å‹
model = EmotionDetector(len(vocab), 128, 256)
print(model)
```

**æœºç”²éƒ¨ä»¶è§£æ**ï¼š
1. **Embeddingå±‚**ï¼šæŠŠå•è¯å˜æˆ128ç»´å‘é‡ï¼ˆæ–‡å­—â†’æ•°å­¦åæ ‡ï¼‰
2. **Bi-LSTM**ï¼šåŒå‘æ‰«æå¥å­ï¼Œæ•æ‰å‰åè¯­å¢ƒï¼ˆåƒåŒæ—¶ç”¨å·¦å³è„‘æ€è€ƒï¼‰
3. **å…¨è¿æ¥å±‚**ï¼šæŠŠLSTMçš„è¾“å‡ºå‹ç¼©æˆ1ä¸ªæ¦‚ç‡å€¼

---

### â…¤. **è®­ç»ƒå‡†å¤‡ï¼šé…ç½®ç‚¼ä¸¹ç‚‰å‚æ•°**
```python
import torch.optim as optim

# æŸå¤±å‡½æ•°ï¼ˆäºŒåˆ†ç±»ä»»åŠ¡ç”¨BCEWithLogitsLossæ›´é«˜æ•ˆï¼‰
criterion = nn.BCEWithLogitsLoss()
# ä¼˜åŒ–å™¨ï¼ˆå¸¦å­¦ä¹ ç‡è¡°å‡çš„Adamï¼‰
optimizer = optim.Adam(model.parameters(), lr=0.001)
# å­¦ä¹ ç‡è°ƒåº¦å™¨
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)

# æŠŠæ¨¡å‹æ‰”åˆ°GPUï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
```

**ç‚¼ä¸¹å£è¯€**ï¼š
- BCELossè‡ªå¸¦Sigmoid â†’ çœå»æ‰‹åŠ¨è®¡ç®—
- StepLR â†’ æ¯5è½®å­¦ä¹ ç‡æ‰“9æŠ˜

---

### â…¥. **è®­ç»ƒå¾ªç¯ï¼šå¯åŠ¨æƒ…ç»ªç†”ç‚‰**
```python
for epoch in range(10):
    model.train()
    total_loss = 0
    for texts, labels in train_loader:
        texts, labels = texts.to(device), labels.float().to(device)
      
        optimizer.zero_grad()
        outputs = model(texts).squeeze()
        loss = criterion(outputs, labels)
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # æ¢¯åº¦è£å‰ª
        optimizer.step()
      
        total_loss += loss.item()
  
    scheduler.step()
    print(f'Epoch: {epoch+1}, Loss: {total_loss/len(train_loader):.4f}')

# ç¤ºä¾‹è¾“å‡ºï¼š
# Epoch: 1, Loss: 0.5321
# Epoch: 2, Loss: 0.3216
# ...
# Epoch: 10, Loss: 0.1124
```

**é¿å‘æŒ‡å—**ï¼š
- `clip_grad_norm_`ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼ˆåƒç»™æ°´ç®¡åŠ å‹åŠ›é˜€ï¼‰
- `squeeze()`ï¼šå»æ‰å¤šä½™çš„ç»´åº¦ï¼ˆæŠŠ[[0.5], [0.7]] â†’ [0.5, 0.7]ï¼‰

---

### â…¦. **æ¨¡å‹éªŒè¯ï¼šAIæƒ…æ„Ÿå¤§å¸ˆè€ƒè¯•**
```python
model.eval()
correct, total = 0, 0

with torch.no_grad():
    for texts, labels in test_loader:
        texts, labels = texts.to(device), labels.to(device)
        outputs = model(texts).squeeze()
        predicted = (torch.sigmoid(outputs) > 0.5).int()
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f'å‡†ç¡®ç‡: {100*correct/total:.2f}%')
# è¾“å‡º: å‡†ç¡®ç‡: 87.32% ï¼ˆç»è¿‡è°ƒå‚å¯è¾¾æ›´é«˜ï¼‰
```

**æ€§èƒ½ä¼˜åŒ–æŠ€å·§**ï¼š
- å°è¯•Transformeræ¨¡å‹ï¼ˆå¦‚BERTï¼‰
- ä½¿ç”¨é¢„è®­ç»ƒè¯å‘é‡ï¼ˆGloVeï¼‰
- å¢åŠ æ³¨æ„åŠ›æœºåˆ¶

---

ğŸ¯ **è¯¾åä¸‰é—®**ï¼š
1. ä¸ºä»€ä¹ˆè¦ç”¨åŒå‘LSTMï¼Ÿ
   â†’ æ­£å‘çœ‹å¥å­çš„å‰åŠæ®µï¼Œåå‘çœ‹ååŠæ®µï¼Œåƒä¸¤äººæ¥åŠ›è¯»è®ºæ–‡

2. å¦‚ä½•å¤„ç†é•¿æ–‡æœ¬ï¼Ÿ
   â†’ æˆªæ–­åˆ°å›ºå®šé•¿åº¦ æˆ– ä½¿ç”¨Transformerï¼ˆå¯å¹¶è¡Œå¤„ç†é•¿åºåˆ—ï¼‰

3. é‡åˆ°è„è¯å¹²æ‰°æ€ä¹ˆåŠï¼Ÿ
   â†’ æ•°æ®æ¸…æ´— æˆ– åœ¨è¯å‘é‡ä¸­ç»™è„è¯ç‰¹æ®Šæ ‡è®°

```python
# å½©è›‹ï¼šå®æ—¶æƒ…ç»ªæµ‹è¯•
def predict(text):
    with torch.no_grad():
        tokenized = torch.tensor(text_pipeline(text)).unsqueeze(0).to(device)
        output = model(tokenized)
        prob = torch.sigmoid(output).item()
        return "ğŸ˜„" if prob > 0.5 else "ğŸ˜ "

print(predict("This movie blew my mind!"))  # è¾“å‡º: ğŸ˜„
print(predict("Waste of time and money"))   # è¾“å‡º: ğŸ˜ 
```

ğŸ”” **ç„å­¦æ€»ç»“**ï¼š
"æƒ…æ„Ÿåˆ†ææ¨¡å‹å°±æ˜¯ä¸ªæ•°å­—æ—¶ä»£çš„è¯»å¿ƒç¥æ¢ï¼š
ğŸ‘‰ è¯å‘é‡æ˜¯å®ƒçš„å¿ƒç†å­¦è¯å…¸
ğŸ‘‰ LSTMæ˜¯å®ƒçš„å¾®è¡¨æƒ…åˆ†æä»ª
ğŸ‘‰ å…¨è¿æ¥å±‚æ˜¯å®ƒçš„æœ€ç»ˆç›´è§‰åˆ¤æ–­ï¼"