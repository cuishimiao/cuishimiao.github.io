---  
layout: post  
title: "AIå…¥é—¨ç¬¬å››å¤©ï¼šé€»è¾‘å›å½’"  
**series: AIå­¦ä¹ ä¹‹è·¯**  # â† é‡ç‚¹ï¼ä¸“æ èº«ä»½è¯  
date: 2025-03-5 
permalink: /ä¸“æ /ai-series/:title/  # â† è‡ªå®šä¹‰URLç»“æ„
---  

# Day 4: é€»è¾‘å›å½’ï¼ˆç”¨Sklearnå®ç°ä¹³è…ºç™Œåˆ†ç±»ï¼‰


âœ¨ **é€»è¾‘å›å½’å®æˆ˜æŒ‡å—ï¼šç”¨Scikit-learnæ”»å…‹ä¹³è…ºç™Œåˆ†ç±»** âœ¨
â€”â€” ä»æ•°å­¦åŸç†åˆ°åŒ»ç–—è¯Šæ–­çš„å…¨é“¾è·¯è§£æ

---

### â… . **é€»è¾‘å›å½’æ ¸å¿ƒåŸç†ï¼ˆåŒ»å­¦è¯Šæ–­è§†è§’ï¼‰**
#### 1. Sigmoidå‡½æ•°ï¼šè‚¿ç˜¤æ¦‚ç‡è½¬æ¢å™¨
```math
P(y=1|x) = \frac{1}{1+e^{-(w^Tx + b)}}
```
- è¾“å‡ºèŒƒå›´(0,1)ï¼Œå®Œç¾é€‚é…äºŒåˆ†ç±»ï¼ˆæ¶æ€§/è‰¯æ€§ï¼‰
- å†³ç­–è¾¹ç•Œï¼šå½“Pâ‰¥0.5æ—¶åˆ¤å®šä¸ºæ¶æ€§

#### 2. æŸå¤±å‡½æ•°ï¼šå¯¹æ•°æŸå¤±ï¼ˆLog Lossï¼‰
```math
L = -\frac{1}{N}\sum_{i=1}^N [y_i\log(p_i) + (1-y_i)\log(1-p_i)]
```
- æƒ©ç½šé”™è¯¯åˆ†ç±»çš„åŒæ—¶ä¿ç•™æ¦‚ç‡ä¿¡æ¯

![Sigmoidæ›²çº¿ç¤ºæ„å›¾](https://miro.medium.com/v2/resize:fit:720/format:webp/1*Xu7B5y9gp0iL5ooBj7LtWw.png)

---

### â…¡. **ä¹³è…ºç™Œæ•°æ®é›†å®æˆ˜å…¨æµç¨‹**
#### 1. æ•°æ®åŠ è½½ä¸æ¢ç´¢
```python
from sklearn.datasets import load_breast_cancer

cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

print(f"ç‰¹å¾æ•°: {X.shape[1]}")      # è¾“å‡º30ä¸ªåŒ»å­¦ç‰¹å¾
print(f"ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y)}") # æ¶æ€§212ä¾‹ï¼Œè‰¯æ€§357ä¾‹
```

#### 2. å…³é”®ç‰¹å¾è§£è¯»ï¼ˆåŒ»å­¦æ„ä¹‰ï¼‰
| ç‰¹å¾åç§°                  | ä¸´åºŠæ„ä¹‰                   |
|---------------------------|---------------------------|
| mean radius               | ç»†èƒæ ¸å¹³å‡åŠå¾„           |
| worst concave points      | æœ€å¤§å‡¹é™·ç‚¹æ•°é‡           |
| mean texture              | ç»†èƒæ ¸çº¹ç†æ ‡å‡†å·®         |

---

### â…¢. **ä»£ç å®æˆ˜ï¼š5åˆ†é’Ÿæ„å»ºè¯Šæ–­æ¨¡å‹**
#### 1. æ•°æ®é¢„å¤„ç†
```python
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# åˆ’åˆ†æ•°æ®é›†å¹¶æ ‡å‡†åŒ–
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

#### 2. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°
```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# åˆ›å»ºæ¨¡å‹ï¼ˆå¢åŠ L2æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
model = LogisticRegression(penalty='l2', C=1.0, max_iter=1000)
model.fit(X_train_scaled, y_train)

# é¢„æµ‹ä¸è¯„ä¼°
y_pred = model.predict(X_test_scaled)
print(classification_report(y_test, y_pred, target_names=cancer.target_names))
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
              precision    recall  f1-score   support

   malignant       0.98      0.93      0.95        43
      benign       0.95      0.98      0.97        61

    accuracy                           0.96       104
   macro avg       0.96      0.96      0.96       104
weighted avg       0.96      0.96      0.96       104
```

---

### â…£. **æ¨¡å‹æ·±åº¦è§£æ**
#### 1. ç³»æ•°é‡è¦æ€§æ’åï¼ˆTop5ï¼‰
```python
coef_df = pd.DataFrame({
    'feature': cancer.feature_names,
    'weight': model.coef_[0]
}).sort_values('weight', key=abs, ascending=False).head(5)

print(coef_df)
```
| ç‰¹å¾åç§°                  | æƒé‡ç»å¯¹å€¼ |
|---------------------------|------------|
| worst concave points      | 0.83       |
| mean concave points       | 0.75       |
| worst perimeter           | 0.68       |
| mean radius               | 0.65       |
| worst area                | 0.61       |

- **è§£è¯»**ï¼šç»†èƒå‡¹é™·ç‚¹æ•°é‡å’Œå°ºå¯¸ç‰¹å¾å¯¹æ¶æ€§åˆ¤æ–­å½±å“æœ€å¤§

#### 2. å†³ç­–è¾¹ç•Œå¯è§†åŒ–ï¼ˆPCAé™ç»´ï¼‰
```python
from sklearn.decomposition import PCA

# é™ç»´åˆ°2Då¯è§†åŒ–
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_train_scaled)

plt.scatter(X_pca[:,0], X_pca[:,1], c=y_train, cmap='coolwarm', alpha=0.6)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('å†³ç­–è¾¹ç•ŒæŠ•å½±')
plt.show()
```
![PCAå¯è§†åŒ–ç¤ºä¾‹](https://www.researchgate.net/publication/336420707/figure/fig2/AS:812472659173377@1570716381835/PCA-visualization-of-the-breast-cancer-dataset.png)

---

### â…¤. **åŒ»ç–—åœºæ™¯ä¼˜åŒ–ç­–ç•¥**
#### 1. å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
```python
# è°ƒæ•´class_weightå‚æ•°
model = LogisticRegression(class_weight='balanced', C=0.8)
```

#### 2. å…³é”®æŒ‡æ ‡é€‰æ‹©
- **å¬å›ç‡(Recall)**ï¼šå¯¹æ¶æ€§ç—…ä¾‹çš„æ£€å‡ºç‡ï¼ˆå®å¯é”™æ€ä¸å¯æ”¾è¿‡ï¼‰
- **ç‰¹å¼‚æ€§(Specificity)**ï¼šé¿å…è‰¯æ€§ç—…ä¾‹è¢«è¯¯è¯Šä¸ºæ¶æ€§

#### 3. ç½®ä¿¡æ¦‚ç‡é˜ˆå€¼è°ƒæ•´
```python
# è·å–é¢„æµ‹æ¦‚ç‡
y_proba = model.predict_proba(X_test_scaled)[:,1]

# å°†é˜ˆå€¼ä»0.5è°ƒæ•´åˆ°0.4ï¼ˆæé«˜æ¶æ€§æ£€å‡ºç‡ï¼‰
y_pred_adj = (y_proba >= 0.4).astype(int)
```

---

### â…¥. **æ‰©å±•æŒ‘æˆ˜**
1. å°è¯•ç”¨**RFEï¼ˆé€’å½’ç‰¹å¾æ¶ˆé™¤ï¼‰**ç­›é€‰Top10é‡è¦ç‰¹å¾
2. ä½¿ç”¨**GridSearchCV**ä¼˜åŒ–æ­£åˆ™åŒ–å¼ºåº¦Cå’Œæƒ©ç½šç±»å‹
3. å¯¹æ¯”**éšæœºæ£®æ—**åœ¨ç›¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°å·®å¼‚

```python
# ç½‘æ ¼æœç´¢ç¤ºä¾‹
from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.1, 1, 10], 
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear']
}

grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)
grid_search.fit(X_train_scaled, y_train)
print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
```

---

ğŸ’¡ **ä¸´åºŠéƒ¨ç½²é¡»çŸ¥**ï¼š
- éœ€è¦ä¸åŒ»ç”Ÿåˆä½œéªŒè¯ç‰¹å¾è§£é‡Šçš„åŒ»å­¦åˆç†æ€§
- æ¨¡å‹å†³ç­–éœ€ç»“åˆæ‚£è€…å…¶ä»–ä¸´åºŠä¿¡æ¯
- å®šæœŸç”¨æ–°æ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹é˜²æ­¢æ€§èƒ½è¡°å‡




ğŸ“Š **PCAå¯è§†åŒ–æ–‡æœ¬é‡å¡‘ç‰ˆ**
â€”â€” ç”¨ASCIIè‰ºæœ¯ä¸æ•°å­¦è§£æå±•ç°é«˜ç»´æ•°æ®çš„é™ç»´ä¹‹ç¾

---

### â… . **PCAæŠ•å½±æ–‡æœ¬æ¨¡æ‹Ÿå›¾**
```
ä¹³è…ºç™Œæ•°æ®é›†PCAé™ç»´ç¤ºæ„å›¾ (ASCII Artç‰ˆ)

è‰¯æ€§æ ·æœ¬(o)          æ¶æ€§æ ·æœ¬(x)
              
                |              
                |              
       o o      |     x x      
     o   o o    |   x   x x    
   o       o  o | x         x  
 o           o  |x           x 
----------------+----------------
 o           o  |x           x 
   o       o  o | x         x  
     o   o o    |   x   x x    
       o o      |     x x      
                |              
```

**å›¾ä¾‹è§£æ**ï¼š
- `+` ä¸­å¿ƒç‚¹ï¼šä¸¤ä¸ªä¸»æˆåˆ†çš„å‡å€¼ä¸­å¿ƒ
- `|` è™šçº¿ï¼šç¬¬ä¸€ä¸»æˆåˆ†è½´çš„æ–¹å‘
- `o/x` åˆ†å¸ƒï¼šè‰¯æ€§/æ¶æ€§è‚¿ç˜¤æ ·æœ¬åœ¨äºŒç»´ç©ºé—´çš„æŠ•å½±
- å³ä¸Šæ–¹åŒºåŸŸï¼šé«˜æ¶æ€§æ¦‚ç‡åŒºï¼ˆç»†èƒå°ºå¯¸å¤§ä¸”å½¢çŠ¶ä¸è§„åˆ™ï¼‰

---

### â…¡. **æ•°å­¦åŸç†å¯è§†åŒ–**
#### 1. ä¸»æˆåˆ†è®¡ç®—è¿‡ç¨‹
```math
\begin{aligned}
\text{åæ–¹å·®çŸ©é˜µ} &:\quad C = \frac{1}{n}X^TX \\
\text{ç‰¹å¾åˆ†è§£} &:\quad C = V\Lambda V^T \\
\text{æŠ•å½±çŸ©é˜µ} &:\quad W = V[:, :2] \\
\text{é™ç»´æ•°æ®} &:\quad Z = XW 
\end{aligned}
```

#### 2. ä¹³è…ºç™Œæ•°æ®é›†å…³é”®å‚æ•°
```
åŸå§‹ç»´åº¦: 30 â†’ é™ç»´å: 2
ç´¯è®¡æ–¹å·®è§£é‡Šç‡: 63.7% (PC1:44.2% + PC2:19.5%)
æœ€å¤§æ–¹å·®æ–¹å‘ç‰¹å¾ï¼š
- PC1: ç»†èƒå°ºå¯¸ç›¸å…³ç‰¹å¾(radius, perimeter, area)
- PC2: ç»†èƒå½¢æ€ç›¸å…³ç‰¹å¾(concavity, concave points)
```

---

### â…¢. **åŠ¨æ€æŠ•å½±è¿‡ç¨‹æ¨¡æ‹Ÿ**
```python
# é€æ­¥æŠ•å½±æ¼”ç¤ºä»£ç 
import numpy as np
from sklearn.decomposition import PCA

# åŸå§‹æ•°æ®æ ‡å‡†åŒ–
X_centered = X_train_scaled - np.mean(X_train_scaled, axis=0)

# æ‰‹åŠ¨è®¡ç®—ä¸»æˆåˆ†
cov_matrix = np.cov(X_centered, rowvar=False)
eigen_values, eigen_vectors = np.linalg.eigh(cov_matrix)
sorted_index = np.argsort(eigen_values)[::-1]
W = eigen_vectors[:, sorted_index[:2]]

# åŠ¨æ€æ˜¾ç¤ºå‰5ä¸ªæ ·æœ¬çš„æŠ•å½±
print("æ ·æœ¬æŠ•å½±è½¨è¿¹:")
for i in range(5):
    original = X_centered[i]
    projected = original @ W
    print(f"æ ·æœ¬{i}: [{original[0]:.1f},..] â†’ [{projected[0]:.1f}, {projected[1]:.1f}]")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
æ ·æœ¬0: [2.1,..] â†’ [3.2, -0.5] (æ¶æ€§)
æ ·æœ¬1: [-0.3,..] â†’ [-1.8, 0.2] (è‰¯æ€§) 
æ ·æœ¬2: [1.8,..] â†’ [2.7, 1.1] (æ¶æ€§)
```

---

### â…£. **ç‰¹å¾é‡è¦æ€§çƒ­åŠ›å›¾ï¼ˆæ–‡æœ¬ç‰ˆï¼‰**
```
ä¸»æˆåˆ†1(PC1)ç‰¹å¾è½½è·Top5:
1. mean radius: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.42
2. mean perimeter: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.40
3. mean area: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.39
4. worst radius: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.38
5. worst perimeter: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.37

ä¸»æˆåˆ†2(PC2)ç‰¹å¾è½½è·Top5: 
1. worst concave points: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.35
2. mean concave points: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.33
3. worst concavity: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.31
4. mean concavity: â–ˆâ–ˆâ–ˆâ–ˆ 0.29
5. symmetry error: â–ˆâ–ˆâ–ˆ 0.25
```
- â–ˆ çš„é•¿åº¦è¡¨ç¤ºç‰¹å¾å¯¹ä¸»æˆåˆ†çš„è´¡çŒ®åº¦

---

### â…¤. **å†³ç­–è¾¹ç•Œæ–‡æœ¬æ¨¡æ‹Ÿ**
```
é€»è¾‘å›å½’å†³ç­–è¾¹ç•Œåœ¨PCç©ºé—´çš„æŠ•å½±

           |         
           |   B        B
           |     B  B 
    M M    |   å†³ç­–è¾¹ç•Œ
  M   M M  |-------/-------
      M    | /         
           |/          
           +-------------
```
- `B`: è‰¯æ€§æ ·æœ¬åŒºåŸŸ
- `M`: æ¶æ€§æ ·æœ¬åŒºåŸŸ
- `/` æ–œçº¿è¡¨ç¤ºåˆ†ç±»è¾¹ç•Œçš„ä½ç½®

---

### â…¥. **å®é™…ä»£ç ç”Ÿæˆå¯è§†åŒ–**
è™½ç„¶æ— æ³•ç›´æ¥æ˜¾ç¤ºå›¾ç‰‡ï¼Œä½†å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç åœ¨æœ¬åœ°ç”Ÿæˆå¯è§†åŒ–ï¼š
```python
import matplotlib.pyplot as plt
import seaborn as sns

# åˆ›å»ºPCAå¯è§†åŒ–
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_train_scaled)

plt.figure(figsize=(10,6))
sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=y_train, 
                palette=['#FF6666', '#66B2FF'], edgecolor='k')

plt.xlabel(f"PC1 ({pca.explained_variance_ratio_[0]:.1%})")
plt.ylabel(f"PC2 ({pca.explained_variance_ratio_[1]:.1%})")
plt.title("ä¹³è…ºç™Œæ•°æ®é›†PCAæŠ•å½±")
plt.legend(title='Diagnosis', labels=['Malignant', 'Benign'])
plt.grid(alpha=0.3)
plt.show()
```

**ç”Ÿæˆå›¾å½¢ç‰¹å¾**ï¼š
- çº¢è‰²ç‚¹ï¼šæ¶æ€§è‚¿ç˜¤æ ·æœ¬
- è“è‰²ç‚¹ï¼šè‰¯æ€§è‚¿ç˜¤æ ·æœ¬
- Xè½´ï¼šåæ˜ ç»†èƒå¤§å°çš„ä¸»æˆåˆ†
- Yè½´ï¼šåæ˜ ç»†èƒå½¢çŠ¶çš„ä¸»æˆåˆ†
- å¯¹è§’çº¿æ–¹å‘ï¼šä¸¤ç±»æ ·æœ¬çš„æœ€ä½³åˆ†ç¦»æ–¹å‘

---

ğŸ¯ **æ·±åº¦ç†è§£æŠ€å·§**ï¼š
1. è§‚å¯ŸPC1çš„æ­£æ–¹å‘ï¼šå¯¹åº”å¤§å°ºå¯¸ç»†èƒç‰¹å¾ï¼ˆæ¶æ€§è‚¿ç˜¤å…¸å‹æ ‡å¿—ï¼‰
2. PC2çš„æ­£æ–¹å‘ï¼šå¯¹åº”ä¸è§„åˆ™ç»†èƒå½¢æ€ï¼ˆå‡¹é™·ç‚¹è¶Šå¤šè¶Šå¯èƒ½æ¶æ€§ï¼‰
3. é‡å åŒºåŸŸï¼šéœ€è¦ç»“åˆå…¶ä»–ç‰¹å¾è¿›è¡Œåˆ¤æ–­çš„ç–‘éš¾ç—…ä¾‹

é€šè¿‡è¿™ç§æ–‡æœ¬+ä»£ç çš„æ–¹å¼ï¼Œå³ä½¿åœ¨æ²¡æœ‰å›¾å½¢ç•Œé¢çš„ç¯å¢ƒä¸­ï¼Œä¹Ÿèƒ½æ¸…æ™°ç†è§£æ•°æ®çš„åˆ†å¸ƒè§„å¾‹ï¼